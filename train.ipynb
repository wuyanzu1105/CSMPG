{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc69d34-101a-4e39-9f97-18423eca4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import torch\n",
    "from dqn_agent_pytorch import DQNAgent\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from env.hgnn import hgnn_env\n",
    "\n",
    "def seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    print(random_seed)\n",
    "\n",
    "def get_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s', \"%Y-%m-%d %H:%M:%S\")\n",
    "    fileHandler = logging.FileHandler(log_file, mode='a')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    return logging.getLogger(logger_name)\n",
    "\n",
    "def use_pretrain(env, dataset='yelp_data'):\n",
    "    if dataset == 'yelp_data':\n",
    "        print('./data/yelp_data/embedding/user.embedding_' + str(env.data.entity_dim))\n",
    "        fr1 = open('./data/yelp_data/embedding/user.embedding_' + str(env.data.entity_dim), 'r')\n",
    "        fr2 = open('./data/yelp_data/embedding/item.embedding_' + str(env.data.entity_dim), 'r')\n",
    "    elif dataset == 'douban_movie':\n",
    "        print('./data/douban_movie/embedding/user.embedding_' + str(env.data.entity_dim))\n",
    "        fr1 = open('./data/douban_movie/embedding/user.embedding_' + str(env.data.entity_dim), 'r')\n",
    "        fr2 = open('./data/douban_movie/embedding/item.embedding_' + str(env.data.entity_dim), 'r')\n",
    "    else:\n",
    "        print('./data/' + dataset + '/embedding/user.embedding_' + str(env.data.entity_dim))\n",
    "        fr1 = open('./data/' + dataset + '/embedding/user.embedding_' + str(env.data.entity_dim), 'r')\n",
    "        fr2 = open('./data/' + dataset + '/embedding/item.embedding_' + str(env.data.entity_dim), 'r')\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    emb = env.train_data.x\n",
    "    emb.requires_grad = False\n",
    "\n",
    "    for line in fr1.readlines():\n",
    "        embeddings = line.strip().split()\n",
    "        id, embedding = int(embeddings[0]), embeddings[1:]\n",
    "        embedding = list(map(float, embedding))\n",
    "        emb[id] = torch.tensor(embedding)\n",
    "\n",
    "    for line in fr2.readlines():\n",
    "        embeddings = line.strip().split()\n",
    "        id, embedding = int(embeddings[0]), embeddings[1:]\n",
    "        embedding = list(map(float, embedding))\n",
    "        emb[id] = torch.tensor(embedding)\n",
    "\n",
    "    env.train_data.x = emb.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa930e-5ae7-4989-a73f-48617382bcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1247f8-cd32-4900-8dd1-a6d4aedb17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run HGNN.\")\n",
    "\n",
    "    parser.add_argument('--local_rank', type=int, default=0,\n",
    "                        help='Local rank for using multi GPUs.')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=123,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--task', nargs='?', default='rec',\n",
    "                        help='Choose a task from {rec, herec, mcrec, classification}')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='TCL',\n",
    "                        help='Choose a dataset from {yelp_data, douban_movie, TCL}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='data/',\n",
    "                        help='Input data path.')\n",
    "\n",
    "    parser.add_argument('--use_pretrain', type=int, default=0,\n",
    "                        help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "    parser.add_argument('--pretrain_embedding_dir', nargs='?', default='datasets/pretrain/',\n",
    "                        help='Path of learned embeddings.')\n",
    "    parser.add_argument('--pretrain_model_path', nargs='?', default='trained_model/model.pth',\n",
    "                        help='Path of stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=90000,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=10000,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--nd_batch_size', type=int, default=5000,\n",
    "                        help='node sampling batch size.')\n",
    "    parser.add_argument('--rl_batch_size', type=int, default=1,\n",
    "                        help='RL training batch size.')\n",
    "    parser.add_argument('--train_batch_size', type=int, default=2000,\n",
    "                        help='Eval batch size (the user number to test every batch).')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=20000,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--entity_dim', type=int, default=64,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=32,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--aggregation_type', nargs='?', default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=1000,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=10,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--limit', type=float, default=1000,\n",
    "                        help='Time Limit.')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=1,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=1,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--K', type=int, default=20,\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "    parser.add_argument('--episode', type=int, default=20,\n",
    "                        help='episode')\n",
    "    parser.add_argument('--feats-type', type=int, default=2,\n",
    "                        help='Type of the node features used. ' +\n",
    "                             '0 - loaded features; ' +\n",
    "                             '1 - only target node features (zero vec for others); ' +\n",
    "                             '2 - only target node features (id vec for others); ' +\n",
    "                             '3 - all id vec. Default is 2.')\n",
    "    parser.add_argument('--layers', type=int, default=2, help='Number of layers. Default is 2.')\n",
    "    parser.add_argument('--hidden-dim', type=int, default=64, help='Dimension of the node hidden state. Default is 64.')\n",
    "    parser.add_argument('--num-heads', type=list, default=[4], help='Number of the attention heads. Default is 8.')\n",
    "    parser.add_argument('--attn-vec-dim', type=int, default=128,\n",
    "                        help='Dimension of the attention vector. Default is 128.')\n",
    "    parser.add_argument('--rnn-type', default='RotatE0', help='Type of the aggregator. Default is RotatE0.')\n",
    "    parser.add_argument('--epoch', type=int, default=100, help='Number of epochs. Default is 100.')\n",
    "    parser.add_argument('--patience', type=int, default=10, help='Patience. Default is 10.')\n",
    "    parser.add_argument('--repeat', type=int, default=1,\n",
    "                        help='Repeat the training and testing for N times. Default is 1.')\n",
    "    parser.add_argument('--log', default='',\n",
    "                        help='Name in log')\n",
    "    parser.add_argument('--mpset', default=\"[[['2', '1']], [['1', '2']]]\",\n",
    "                        help='Meta-path Set.')\n",
    "    parser.add_argument('--init', default=\"RL\",\n",
    "                        help='Meta-path Set initialization method.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    save_dir = 'trained_model/HGNN/{}/entitydim{}_relationdim{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        args.data_name, args.entity_dim, args.relation_dim, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr, args.use_pretrain)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71794d-dfe1-46b3-8485-b23e5a0056e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a721ebe-13c2-418e-9dcf-2b4d2d80ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(args):\n",
    "    seed(0)\n",
    "    print(\"Seed set to 0\\n\")\n",
    "    tim1 = time.time()\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    dataset = args.data_name\n",
    "    max_timesteps = 4 if args.task == 'rec' else 3\n",
    "    if args.task == 'mcrec':\n",
    "        max_timesteps = 5\n",
    "\n",
    "    infor = 'rl_' + str(args.data_name) + '_' + str(args.task) + '_' + str(args.log)\n",
    "    model_name = 'model/' + infor + '.pth'\n",
    "\n",
    "    episode = int(args.episode)\n",
    "    u_max_episodes = episode\n",
    "    i_max_episodes = episode\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    logger1 = get_logger('log', 'log/logger_' + infor + '.log')\n",
    "    logger2 = get_logger('log2', 'log/logger2_' + infor + '.log')\n",
    "\n",
    "    env = hgnn_env(logger1, logger2, model_name, args, dataset=dataset)\n",
    "    use_pretrain(env, dataset)\n",
    "\n",
    "    user_agent = DQNAgent(scope='dqn',\n",
    "                          action_num=env.action_num,\n",
    "                          replay_memory_size=int(1e4),\n",
    "                          replay_memory_init_size=500,\n",
    "                          norm_step=1,\n",
    "                          batch_size=1,\n",
    "                          state_shape=env.obs.shape,\n",
    "                          mlp_layers=[32, 64, 32],\n",
    "                          learning_rate=0.001,\n",
    "                          device=torch.device(device)\n",
    "                          )\n",
    "    env.user_policy = user_agent\n",
    "    best_user_val = 0.0\n",
    "    best_user_i = 0\n",
    "\n",
    "    for i_episode in range(1, u_max_episodes + 1):\n",
    "        env.reset_past_performance()\n",
    "        loss, reward, (val_acc, reward) = user_agent.user_learn(logger1, logger2, env, max_timesteps)\n",
    "        logger2.info(\"Generated meta-path set: %s\" % str(env.etypes_lists))\n",
    "        print(\"Generated meta-path set: %s\\n\" % str(env.etypes_lists))\n",
    "        if val_acc > best_user_val:\n",
    "            best_user_policy = deepcopy(user_agent)\n",
    "            best_user_val = val_acc\n",
    "            best_user_i = i_episode\n",
    "        logger2.info(\"Training User Meta-policy: %d    Val_Acc: %.5f    Avg_reward: %.5f    Best_Acc:  %.5f    Best_i: %d \"\n",
    "                     % (i_episode, val_acc, reward, best_user_val, best_user_i))\n",
    "        logger2.info(\"\")  # 添加空行\n",
    "        print(\"\")  # 添加空行\n",
    "        for i in range(4):\n",
    "            user_agent.train()\n",
    "\n",
    "    tim_1 = time.time()\n",
    "    for i in range(50):\n",
    "        user_agent.train()\n",
    "\n",
    "    if args.task != 'mcrec':\n",
    "        item_agent = DQNAgent(scope='dqn',\n",
    "                              action_num=env.action_num,\n",
    "                              replay_memory_size=int(1e4),\n",
    "                              replay_memory_init_size=500,\n",
    "                              norm_step=1,\n",
    "                              batch_size=1,\n",
    "                              state_shape=env.obs.shape,\n",
    "                              mlp_layers=[32, 64, 32],\n",
    "                              learning_rate=0.001,\n",
    "                              device=torch.device(device)\n",
    "                              )\n",
    "        env.item_policy = item_agent\n",
    "        best_item_val = 0.0\n",
    "        best_item_i = 0\n",
    "\n",
    "        logger2.info(\"Training Meta-policy on Validation Set\")\n",
    "\n",
    "        for i_episode in range(1, i_max_episodes + 1):\n",
    "            env.reset_past_performance()\n",
    "            loss, reward, (val_acc, reward) = item_agent.item_learn(logger1, logger2, env, max_timesteps)\n",
    "            logger2.info(\"Generated meta-path set: %s\" % str(env.etypes_lists))\n",
    "            print(\"Generated meta-path set: %s\\n\" % str(env.etypes_lists))\n",
    "            if val_acc > best_item_val:\n",
    "                best_item_policy = deepcopy(item_agent)\n",
    "                best_item_val = val_acc\n",
    "                best_item_i = i_episode\n",
    "            logger2.info(\"Training Item Meta-policy: %d    Val_Acc: %.5f    Avg_reward: %.5f    Best_Acc:  %.5f    Best_i: %d \"\n",
    "                         % (i_episode, val_acc, reward, best_item_val, best_item_i))\n",
    "            logger2.info(\"\")  # 添加空行\n",
    "            print(\"\")  # 添加空行\n",
    "            for i in range(4):\n",
    "                item_agent.train()\n",
    "        for i in range(50):\n",
    "            item_agent.train()\n",
    "\n",
    "    print('Reinforced training time: ', time.time() - tim_1, 's\\n')\n",
    "\n",
    "    tim2 = time.time()\n",
    "\n",
    "    print(\"RL agent training time: \", (tim2 - tim1) / 60, \"min\\n\")\n",
    "\n",
    "    os.makedirs('model', exist_ok=True)\n",
    "    torch.save({'q_estimator_qnet_state_dict': env.user_policy.q_estimator.qnet.state_dict(),\n",
    "                'target_estimator_qnet_state_dict': env.user_policy.target_estimator.qnet.state_dict(),\n",
    "                'Val': best_user_val},\n",
    "               'model/a-best-user-' + str(best_user_val) + '-' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + '.pth.tar')\n",
    "\n",
    "    if args.task != 'mcrec':\n",
    "        os.makedirs('model', exist_ok=True)\n",
    "        torch.save({'q_estimator_qnet_state_dict': env.item_policy.qnet.state_dict(),\n",
    "                    'target_estimator_qnet_state_dict': env.item_policy.target_estimator.qnet.state_dict(),\n",
    "                    'Val': best_item_val},\n",
    "                   'model/a-best-item-' + str(best_item_val) + '-' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + '.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c926067-38be-406c-80aa-242d3ee8f2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfa4c92-9e7c-4aa0-a1f8-a996bc3694ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(args):\n",
    "    # 这里实现测试逻辑，加载模型并在测试数据集上进行评估\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b3835-1473-49b2-92c3-e6489a1f11fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f86977-2694-405a-949d-d323fea8a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--local_rank LOCAL_RANK] [--seed SEED]\n",
      "                             [--task [TASK]] [--data_name [DATA_NAME]]\n",
      "                             [--data_dir [DATA_DIR]]\n",
      "                             [--use_pretrain USE_PRETRAIN]\n",
      "                             [--pretrain_embedding_dir [PRETRAIN_EMBEDDING_DIR]]\n",
      "                             [--pretrain_model_path [PRETRAIN_MODEL_PATH]]\n",
      "                             [--cf_batch_size CF_BATCH_SIZE]\n",
      "                             [--kg_batch_size KG_BATCH_SIZE]\n",
      "                             [--nd_batch_size ND_BATCH_SIZE]\n",
      "                             [--rl_batch_size RL_BATCH_SIZE]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--test_batch_size TEST_BATCH_SIZE]\n",
      "                             [--entity_dim ENTITY_DIM]\n",
      "                             [--relation_dim RELATION_DIM]\n",
      "                             [--aggregation_type [AGGREGATION_TYPE]]\n",
      "                             [--conv_dim_list [CONV_DIM_LIST]]\n",
      "                             [--mess_dropout [MESS_DROPOUT]]\n",
      "                             [--kg_l2loss_lambda KG_L2LOSS_LAMBDA]\n",
      "                             [--cf_l2loss_lambda CF_L2LOSS_LAMBDA] [--lr LR]\n",
      "                             [--n_epoch N_EPOCH]\n",
      "                             [--stopping_steps STOPPING_STEPS] [--limit LIMIT]\n",
      "                             [--cf_print_every CF_PRINT_EVERY]\n",
      "                             [--kg_print_every KG_PRINT_EVERY]\n",
      "                             [--evaluate_every EVALUATE_EVERY] [--K K]\n",
      "                             [--episode EPISODE] [--feats-type FEATS_TYPE]\n",
      "                             [--layers LAYERS] [--hidden-dim HIDDEN_DIM]\n",
      "                             [--num-heads NUM_HEADS]\n",
      "                             [--attn-vec-dim ATTN_VEC_DIM]\n",
      "                             [--rnn-type RNN_TYPE] [--epoch EPOCH]\n",
      "                             [--patience PATIENCE] [--repeat REPEAT]\n",
      "                             [--log LOG] [--mpset MPSET] [--init INIT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e92d8e65-a473-4a2c-9787-c8f27c51a527.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        args = parse_args()\n",
    "        best_user_val, best_item_val = train_model(args)\n",
    "        print(f\"Best User Val: {best_user_val}, Best Item Val: {best_item_val}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08d459-0177-4f38-b4cc-7c75eecf9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    " ################################################################\n",
    "    #max_timesteps = 4 if args.task == 'rec' else 3\n",
    "   # if args.task == 'mcrec':\n",
    "     #   max_timesteps = 5\n",
    "        \n",
    "        \n",
    "    # 调试阶段，使用较小的时间步数和训练周期数\n",
    "    max_timesteps = 2 if args.task == 'rec' else 1\n",
    "    if args.task == 'mcrec':\n",
    "        max_timesteps = 3\n",
    "\n",
    "    episode = int(args.episode) // 10  # 将训练周期数减少10倍\n",
    "    u_max_episodes = episode\n",
    "    i_max_episodes = episode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
